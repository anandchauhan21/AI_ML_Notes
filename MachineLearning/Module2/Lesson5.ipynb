{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGJ3Ft1fRuih+mMUQuR04M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anandchauhan21/AI_ML_Notes/blob/main/MachineLearning/Module2/Lesson5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Lesson 5: Calculus for Optimization\n",
        "\n",
        "## üéØ Objectives\n",
        "By the end of this lesson, you will be able to:\n",
        "- Understand the role of **calculus in Machine Learning**\n",
        "- Compute **derivatives** and **gradients**\n",
        "- Understand **gradient descent**\n",
        "- Implement a simple optimizer in Python\n"
      ],
      "metadata": {
        "id": "JbI30p_3mZch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Concept Recap ‚Äî Calculus in ML\n",
        "\n",
        "Machine Learning models learn by **minimizing a loss function**.\n",
        "Calculus helps us find **how to change parameters** to reduce error.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ Why Calculus in ML?\n",
        "- Measures how **error changes**\n",
        "- Helps find the **direction of steepest descent**\n",
        "- Used in **training neural networks**\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ Key Concepts\n",
        "- Derivative\n",
        "- Partial Derivative\n",
        "- Gradient\n",
        "- Loss Function\n",
        "- Gradient Descent\n"
      ],
      "metadata": {
        "id": "K9uCqT8xmb_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.linspace(-10, 10, 100)\n",
        "y = x**2\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.title(\"Loss Function: y = x¬≤\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7Ya626ASmd9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìê Derivative\n",
        "\n",
        "Derivative = **rate of change**\n",
        "\n",
        "For f(x) = x¬≤  \n",
        "Derivative f'(x) = 2x\n"
      ],
      "metadata": {
        "id": "4JqMjWg3mgP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "    return x**2\n",
        "\n",
        "def df(x):\n",
        "    return 2*x\n",
        "\n",
        "print(\"f(3) =\", f(3))\n",
        "print(\"f'(3) =\", df(3))"
      ],
      "metadata": {
        "id": "dNGnsRyBmij4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîª Gradient Descent\n",
        "\n",
        "Gradient Descent updates parameters using:\n",
        "\n",
        "w_new = w_old - Œ± * gradient\n",
        "\n",
        "Where:\n",
        "- Œ± = learning rate\n",
        "- gradient = derivative of loss\n"
      ],
      "metadata": {
        "id": "UKoZCAWxmk5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = 10\n",
        "lr = 0.1\n",
        "\n",
        "for i in range(10):\n",
        "    grad = df(w)\n",
        "    w = w - lr * grad\n",
        "    print(f\"Step {i+1}: w = {w:.4f}, Loss = {f(w):.4f}\")"
      ],
      "metadata": {
        "id": "mn7uQvqamm-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d973c3f9-cc22-4fbb-eed6-27f8f0212fe3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: w = 8.0000, Loss = 64.0000\n",
            "Step 2: w = 6.4000, Loss = 40.9600\n",
            "Step 3: w = 5.1200, Loss = 26.2144\n",
            "Step 4: w = 4.0960, Loss = 16.7772\n",
            "Step 5: w = 3.2768, Loss = 10.7374\n",
            "Step 6: w = 2.6214, Loss = 6.8719\n",
            "Step 7: w = 2.0972, Loss = 4.3980\n",
            "Step 8: w = 1.6777, Loss = 2.8147\n",
            "Step 9: w = 1.3422, Loss = 1.8014\n",
            "Step 10: w = 1.0737, Loss = 1.1529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w_values = []\n",
        "loss_values = []\n",
        "\n",
        "w = 8\n",
        "for i in range(15):\n",
        "    w_values.append(w)\n",
        "    loss_values.append(f(w))\n",
        "    w = w - 0.1 * df(w)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.scatter(w_values, loss_values, color='red')\n",
        "plt.title(\"Gradient Descent Optimization\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AW8l_yIEmqQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# f(x, y) = x^2 + y^2\n",
        "def f2(x, y):\n",
        "    return x**2 + y**2\n",
        "\n",
        "def grad(x, y):\n",
        "    return 2*x, 2*y\n",
        "\n",
        "x, y = 4, 5\n",
        "dx, dy = grad(x, y)\n",
        "print(\"Gradient at (4,5):\", dx, dy)\n"
      ],
      "metadata": {
        "id": "7P4O8EuymuBp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e16d00c-3086-43a1-a57a-40450c5019cb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient at (4,5): 8 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = float(input(\"Enter x: \"))\n",
        "print(\"f(x) =\", f(x))\n",
        "print(\"df(x) =\", df(x))\n"
      ],
      "metadata": {
        "id": "V6WaoAmdmvxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß© Lesson Recap\n",
        "\n",
        "You learned:\n",
        "‚úÖ Why calculus is needed in ML  \n",
        "‚úÖ What derivatives and gradients are  \n",
        "‚úÖ How gradient descent works  \n",
        "‚úÖ Implemented a basic optimizer  \n",
        "\n",
        "---\n",
        "\n",
        "### üîú Next Lesson:\n",
        "**Lesson 6: Probability & Statistics for ML**\n"
      ],
      "metadata": {
        "id": "pYQm7GR5mxeE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üí¨ Viva Questions\n",
        "\n",
        "1. Why is calculus important in ML?\n",
        "2. What is a gradient?\n",
        "3. What is gradient descent?\n",
        "4. What does learning rate mean?\n",
        "5. What happens if learning rate is too high?\n"
      ],
      "metadata": {
        "id": "cCw5L21qmzdU"
      }
    }
  ]
}